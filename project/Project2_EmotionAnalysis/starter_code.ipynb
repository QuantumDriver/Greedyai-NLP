{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10-final"
    },
    "colab": {
      "name": "starter_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTV1VFzRYoWm",
        "colab_type": "text"
      },
      "source": [
        "## Part 1： 模型理论与应用\n",
        "在第一部分主要以证明和推导为主。以下几个问题都是比较经典的问题，会对模型的深入理解会有很大的帮助。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fdtQT5TYoWp",
        "colab_type": "text"
      },
      "source": [
        "### Part 1.1 逻辑回归相关 (30分)\n",
        "假设我们有训练数据$D=\\{(\\mathbf{x}_1,y_1),...,(\\mathbf{x}_n,y_n)\\}$, 其中$(\\mathbf{x}_i,y_i)$为每一个样本，而且$\\mathbf{x}_i$是样本的特征并且$\\mathbf{x}_i\\in \\mathcal{R}^D$, $y_i$代表样本数据的标签（label）, 取值为$0$或者$1$. 在逻辑回归中，模型的参数为$(\\mathbf{w},b)$。对于向量，我们一般用粗体来表达。请回答以下问题。最好用Markdown自带的Latex来编写。（如果实在不行，可以手写然后拍照完放入word或者转成PDF，作为独立的文件来提交）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8gDhch6YoWq",
        "colab_type": "text"
      },
      "source": [
        "(a) 在逻辑回归模型下，请写出目标函数（objective function）, 也就是我们需要\"最小化\"的目标（也称之为损失函数或者loss function)，不需要考虑正则 （3分）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzZ4tWtRYoWr",
        "colab_type": "text"
      },
      "source": [
        "$L(\\mathbf{w},b)=$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI9VOhwbYoWs",
        "colab_type": "text"
      },
      "source": [
        "(b) 求出$L(\\mathbf{w},b)$的梯度（或者计算导数），需要必要的中间过程。（5分）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drs0Kg7hYoWt",
        "colab_type": "text"
      },
      "source": [
        "$\\frac{\\partial L(\\mathbf{w},b)}{\\partial \\mathbf{w}}=$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fMfEISEYoWu",
        "colab_type": "text"
      },
      "source": [
        "$\\frac{\\partial L(\\mathbf{w},b)}{\\partial b}=$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuSi6o5YoWu",
        "colab_type": "text"
      },
      "source": [
        "(c) 请写出基于梯度下降法（batch）的对于$\\mathbf{w}$和$b$的更新 （5分）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArrHMHrWYoWv",
        "colab_type": "text"
      },
      "source": [
        "$w^{t+1}=$\n",
        "\n",
        "$b^{t+1}=$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDnWrSyWYoWw",
        "colab_type": "text"
      },
      "source": [
        "(d) 假设在(a)的基础上加了一个L2正则项，请写出基于梯度下降法（batch）的对于$\\mathbf{w}$和$b$的更新 （5分）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7NxLpfwYoWx",
        "colab_type": "text"
      },
      "source": [
        "$w^{t+1}=$\n",
        "\n",
        "$b^{t+1}=$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZGKW496YoWy",
        "colab_type": "text"
      },
      "source": [
        "接下来我们来证明逻辑回归函数是凸函数。假设一个函数是凸函数，我们则可以得出局部最优解即为全局最优解，所以假设我们通过随机梯度下降法等手段找到最优解\n",
        "时我们就可以确认这个解就是全局最优解。证明凸函数的方法有很多种，在这里我们介绍一种方法，就是基于二次求导大于等于0。比如给定一个函数$f(x)=x^2-3x+3$，做两次\n",
        "求导之后即可以得出$f''(x)=2 > 0$，所以这个函数就是凸函数。类似的，这种理论也应用于多元变量中的函数上。在多元函数上，只要证明二阶导数是posititive semidefinite即可以。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui6Di8C8YoWz",
        "colab_type": "text"
      },
      "source": [
        "(e) 在(b)的基础上接着对$\\mathbf{w}$求导（等于二阶导数，二阶导数的维度为$D\\times D$），这个二阶导数也称之为Hessian Matrix(https://en.wikipedia.org/wiki/Hessian_matrix) 对于矩阵、向量的求导请参考：https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf （8分）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc-0SjGQYoW0",
        "colab_type": "text"
      },
      "source": [
        "$\\frac{\\partial^2 \\mathcal{L}}{\\partial^2 \\mathbf{w}}=$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca3QnIP_YoW0",
        "colab_type": "text"
      },
      "source": [
        "(f) 请说明在(e)的得出来的Hessian Matrix是Positive Definite. 提示：为了证明一个$D\\times D$的矩阵$H$为Positive Semidefinite，需要证明对于任意一个非零向量$v\\in \\mathcal{R}^D$, 需要得出$v^{T}Hv >=0$ （4分）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9MD1kkWYoW1",
        "colab_type": "text"
      },
      "source": [
        "请推导或者说明："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7yZS2ShYoW2",
        "colab_type": "text"
      },
      "source": [
        "## Part 2： 情感分析项目 (70分)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zWRANZYoW3",
        "colab_type": "text"
      },
      "source": [
        "本项目的目标是基于用户提供的评论，通过算法自动去判断其评论是正面的还是负面的情感。比如给定一个用户的评论：\n",
        "- 评论1： “我特别喜欢这个电器，我已经用了3个月，一点问题都没有！”\n",
        "- 评论2： “我从这家淘宝店卖的东西不到一周就开始坏掉了，强烈建议不要买，真实浪费钱”\n",
        "\n",
        "对于这两个评论，第一个明显是正面的，第二个是负面的。 我们希望搭建一个AI算法能够自动帮我们识别出评论是正面还是负面。\n",
        "\n",
        "情感分析的应用场景非常丰富，也是NLP技术在不同场景中落地的典范。比如对于一个证券领域，作为股民，其实比较关注舆论的变化，这个时候如果能有一个AI算法自动给网络上的舆论做正负面判断，然后把所有相关的结论再整合，这样我们可以根据这些大众的舆论，辅助做买卖的决策。 另外，在电商领域评论无处不在，而且评论已经成为影响用户购买决策的非常重要的因素，所以如果AI系统能够自动分析其情感，则后续可以做很多有意思的应用。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lchx-2YDYoW4",
        "colab_type": "text"
      },
      "source": [
        "情感分析是文本处理领域经典的问题。整个系统一般会包括几个模块：\n",
        "- 数据的抓取： 通过爬虫的技术去网络抓取相关文本数据\n",
        "- 数据的清洗/预处理：在本文中一般需要去掉无用的信息，比如各种标签（HTML标签），标点符号，停用词等等\n",
        "- 把文本信息转换成向量： 这也成为特征工程，文本本身是不能作为模型的输入，只有数字（比如向量）才能成为模型的输入。所以进入模型之前，任何的信号都需要转换成模型可识别的数字信号（数字，向量，矩阵，张量...)\n",
        "- 选择合适的模型以及合适的评估方法。 对于情感分析来说，这是二分类问题（或者三分类：正面，负面，中性），所以需要采用分类算法比如逻辑回归，朴素贝叶斯，神经网络，SVM等等。另外，我们需要选择合适的评估方法，比如对于一个应用，我们是关注准确率呢，还是关注召回率呢？ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEtlveeoYoW4",
        "colab_type": "text"
      },
      "source": [
        "在本次项目中，我们已经给定了训练数据和测试数据，它们分别是 train.positive.txt, train.negative.txt， test_combined.txt. 请注意训练数据和测试数据的格式不一样，详情请见文件内容。 整个项目你需要完成以下步骤：\n",
        "\n",
        "数据的读取以及清洗： 从给定的.txt中读取内容，并做一些数据清洗，这里需要做几个工作： （1） 文本的读取，需要把字符串内容读进来。 （2）去掉无用的字符比如标点符号，多余的空格，换行符等 （3） 分词\n",
        "把文本转换成TF-IDF向量： 这部分直接可以利用sklearn提供的TfidfVectorizer类来做。\n",
        "利用逻辑回归模型来做分类，并通过交叉验证选择最合适的超参数\n",
        "利用支持向量机做分类，并通过交叉验证选择神经网络的合适的参数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfExJs1eYoW5",
        "colab_type": "text"
      },
      "source": [
        "### File Reading: 文本读取 （5分）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns55ydyaYoW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import jieba\n",
        "import numpy as np\n",
        "\n",
        "def process_line(line):   \n",
        "    new_line = re.sub('([a-zA-Z0-9])','',line)      # 正则表达\n",
        "    new_line = ''.join(e for e in new_line if e.isalnum())      # isalnum检测字符串是否由字母和数字组成\n",
        "    new_line = ','.join(jieba.cut(new_line))\n",
        "    return new_line\n",
        "    \n",
        "def process_train(file_path):\n",
        "    comments = []  # 用来存储评论\n",
        "    labels = []    # 用来存储标签（正/负），如果是train_positive.txt，则所有标签为1， 否则0. \n",
        "    with open(file_path) as file:\n",
        "        # TODO 提取每一个评论，然后利用process_line函数来做处理，并添加到comments。\n",
        "        text = file.read().replace(' ','').replace('\\n','')\n",
        "        reg = '<reviewid=.*?</review>'\n",
        "        result = re.findall(reg,text)       # 返回string中所有与pattern相匹配的全部字串，返回形式为数组\n",
        "        for r in result:\n",
        "            r = process_line(r)\n",
        "            comments.append(r)\n",
        "            if file_path == 'train.positive.txt':\n",
        "                labels.append('1')\n",
        "            else:\n",
        "                labels.append('0')\n",
        "    return comments, labels\n",
        "    \n",
        "    \n",
        "def process_test(file_path):\n",
        "    comments = []  # 用来存储评论\n",
        "    labels = []    # 用来存储标签(正/负).\n",
        "    with open(file_path) as file:\n",
        "        # TODO 提取每一个评论，然后利用process_line函数来做处理，并添加到\n",
        "        # comments。\n",
        "        text = file.read().replace(' ','').replace('\\n','')\n",
        "        reg = '<reviewid=.*?</review>'\n",
        "        result = re.findall(reg,text)\n",
        "        for r in result:\n",
        "            \n",
        "            label = re.findall('label=\"(\\d)\"',r)[0]\n",
        "            labels.append(label)\n",
        "            r = process_line(r)\n",
        "            comments.append(r)\n",
        "    return comments, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOgFNhzvYoXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file():\n",
        "    \"\"\"\n",
        "    读取所提供的.txt文件，并把内容处理之后写到list里面。 这里需要分别处理四个文件，“train_positive.txt\", \"train_negative.txt\",\n",
        "    \"test_combined.txt\" 并把每一个文件里的内容存储成列表。 \n",
        "    \"\"\"\n",
        "    # 处理训练数据，这两个文件的格式相同，请指定训练文件的路径\n",
        "    train_pos_comments, train_pos_labels = process_train(\"train.positive.txt\")\n",
        "    train_neg_comments, train_neg_labels = process_train(\"train.negative.txt\")\n",
        "    \n",
        "    # TODO: train_pos_comments和train_neg_comments合并成train_comments， train_pos_labels和train_neg_labels合并成train_labels\n",
        "    train_comments = train_pos_comments + train_neg_comments\n",
        "    train_labels = train_pos_labels + train_neg_labels\n",
        "    # 处理测试数据, 请指定测试文件的路径\n",
        "    test_comments, test_labels = process_test(\"test.combined.txt\")\n",
        "    \n",
        "    return train_comments, train_labels, test_comments, test_labels, train_pos_comments, train_pos_labels, train_neg_comments, train_neg_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhSTnLm3YoXE",
        "colab_type": "text"
      },
      "source": [
        "### Explorary Analysis: 做一些简单的可视化分析 （10分） "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXMAYtcaYoXF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25134397-0f55-4bd4-e3ac-7218f700de10"
      },
      "source": [
        "# 读取数据，并对文本进行处理\n",
        "train_comments, train_labels, test_comments, test_labels, train_pos_comments, train_pos_labels, train_neg_comments, train_neg_labels = read_file()\n",
        "\n",
        "# 查看训练数据与测试数据大小\n",
        "print (len(train_comments), len(train_labels), len(test_comments), len(test_labels))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8065 8065 2500 2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK7eRPo7YoXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: 对于训练数据中的正负样本，分别画出一个histogram， histogram的x抽是每一个样本中字符串的长度，y轴是拥有这个长度的样本的百分比。\n",
        "#       并说出样本长度是否对情感有相关性 (需要先用到结巴分词)\n",
        "#       参考：https://en.wikipedia.org/wiki/Histogram\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enBrPlijYoXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO： 分别列出训练数据中正负样本里的top 20单词（可以做适当的stop words removal）。 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_itb9xsYoXP",
        "colab_type": "text"
      },
      "source": [
        "##### Text Cleaning: 文本处理部分 （10分）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL69srkxYoXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO：对于train_comments, test_comments进行字符串的处理，几个考虑的点：\n",
        "#   1. 停用词过滤\n",
        "#   2. 去掉特殊符号\n",
        "#   3. 去掉数字（比如价格..)\n",
        "#   4. ...\n",
        "#   需要注意的点是，由于评论数据本身很短，如果去掉的太多，很可能字符串长度变成0\n",
        "#   预处理部分，可以自行选择合适的方.\n",
        "train_comments_new = [] \n",
        "test_comments_new = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl-2iCkyYoXS",
        "colab_type": "text"
      },
      "source": [
        "### Feature Extraction : 从文本中提取特征 （10分）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2ufflOFc9Zo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26dcc44f-6ccd-4c62-e20d-df19f9d11ae3"
      },
      "source": [
        "# 把每一个文本内容转换成tf-idf向量\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # 导入sklearn库\n",
        "# TODO: 利用TfidfVectorizer把train_comments转换成tf-idf，把结果存储在X_train, 这里X_train是稀疏矩阵（Sparse Matrix） \n",
        "# 并把train_labels转换成向量 y_train. 类似的，去创建X_test, y_test。 把文本转换成tf-idf过程请参考TfidfVectorizer的说明\n",
        "tfid_vec = TfidfVectorizer()\n",
        "X_train = tfid_vec.fit_transform(train_comments)\n",
        "y_train = np.array(train_labels)\n",
        "X_test = tfid_vec.transform(test_comments)\n",
        "y_test = np.array(test_labels)\n",
        "# 查看每个矩阵，向量的大小, 保证X_train和y_train, X_test和y_test的长度是一样的。\n",
        "print (np.shape(X_train), np.shape(y_train), np.shape(X_test), np.shape(y_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8065, 23876) (8065,) (2500, 23876) (2500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eITLlfFYoXW",
        "colab_type": "text"
      },
      "source": [
        "### Modeling: 训练模型以及选择合适的超参数 （25分）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAFh0_D9eH5X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b24e73a5-c863-4eb6-f1d1-c063b7224eee"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "# TODO： 初始化模型model，并利用模型的fit函数来做训练，暂时用默认的设置。\n",
        "# 具体使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "lr = LogisticRegression().fit(X_train,y_train)\n",
        "# 打印在训练数据上的准确率\n",
        "print (\"训练数据上的准确率为：\" + str(lr.score(X_train, y_train)))\n",
        "\n",
        "# 打印在测试数据上的准确率\n",
        "print (\"测试数据上的准确率为: \" + str(lr.score(X_test, y_test)))\n",
        "\n",
        "# TODO： 打印混淆矩阵（confusion matrix）。\n",
        "# 参考：http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "# 混淆矩阵是一种常用的分析方法。比如模型的准确率不理想的情况下，可以做进一步地分析，并找出原因。 在情感分析问题上，\n",
        "# 混淆矩阵可以用来分析有多少个原本正样本被分类成负样本，有多少原本是负样本的被分类成正样本，可以做这种精细化的结果分析，从而找到一些原因。 \n",
        "\n",
        "# TODO: 利用自己提出的例子来做测试。随意指定一个评论，接着利用process_line来做预处理，再利用之前构建好的TfidfVectorizer来把文本转换\n",
        "# 成tf-idf向量， 然后再利用构建好的model做预测（model.predict函数）\n",
        "test_comment1 = \"这个很好\"\n",
        "test_comment2 = \"垃圾\"\n",
        "test_comment3 = \"评论区说不烂都是骗人的，超赞\"\n",
        "\n",
        "a = []\n",
        "a.append(process_line(test_comment1))\n",
        "print(a)\n",
        "print(tfid_vec.transform(a))\n",
        "print(lr.predict(tfid_vec.transform(a)))\n",
        "\n",
        "# TODO: 输出结果，并自己分析一下是不是跟自己想象的结果一致。 也可以输出两个分类的概率"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "训练数据上的准确率为：0.8683199008059517\n",
            "测试数据上的准确率为: 0.7268\n",
            "['这个,很,好']\n",
            "  (0, 21546)\t1.0\n",
            "['1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev-16I97ebZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "20d0414d-2403-45b2-c245-be6dc27720d7"
      },
      "source": [
        "# 利用决策树来做情感分析预测\n",
        "from sklearn import tree\n",
        "# 具体使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "\n",
        "# TODO: 初始化决策树模型，并利用模型的fit函数来做训练并打印在训练和测试数据上的准确率，利用决策树默认的参数设置\n",
        "dtc1 = tree.DecisionTreeClassifier().fit(X_train,y_train)\n",
        "# 打印在训练数据上的准确率\n",
        "print (\"训练数据上的准确率为：\" + str(dtc1.score(X_train, y_train)))\n",
        "\n",
        "# 打印在测试数据上的准确率\n",
        "print (\"测试数据上的准确率为: \" + str(dtc1.score(X_test, y_test)))\n",
        "\n",
        "\n",
        "# TODO: 初始化决策树模型，并利用模型的fit函数来做训练并打印在训练和测试数据上的准确率，设置max_depth参数为3\n",
        "dtc2 = tree.DecisionTreeClassifier(max_depth=3).fit(X_train,y_train)\n",
        "# 打印在训练数据上的准确率\n",
        "print (\"训练数据上的准确率为：\" + str(dtc2.score(X_train, y_train)))\n",
        "\n",
        "# 打印在测试数据上的准确率\n",
        "print (\"测试数据上的准确率为: \" + str(dtc2.score(X_test, y_test)))\n",
        "\n",
        "\n",
        "# TODO: 初始化决策树模型，并利用模型的fit函数来做训练并打印在训练和测试数据上的准确率，设置max_depth参数为5\n",
        "dtc3 = tree.DecisionTreeClassifier(max_depth=5).fit(X_train,y_train)\n",
        "# 打印在训练数据上的准确率\n",
        "print (\"训练数据上的准确率为：\" + str(dtc3.score(X_train, y_train)))\n",
        "\n",
        "# 打印在测试数据上的准确率\n",
        "print (\"测试数据上的准确率为: \" + str(dtc3.score(X_test, y_test)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "训练数据上的准确率为：0.9954122752634842\n",
            "测试数据上的准确率为: 0.6532\n",
            "训练数据上的准确率为：0.6344699318040917\n",
            "测试数据上的准确率为: 0.5136\n",
            "训练数据上的准确率为：0.6615003099814011\n",
            "测试数据上的准确率为: 0.5468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHLda9O2YoXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e1666b90-6c39-4825-ab06-04d5472d7bf1"
      },
      "source": [
        "# 利用支持向量机（SVM）来做情感分析预测\n",
        "from sklearn import svm\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "# TODO: 初始化SVM模型，并利用模型的fit函数来做训练并打印在训练和测试数据上的准确率，SVM模型的kernel设置成“rbf”核函数\n",
        "svc = svm.SVC(kernel='rbf').fit(X_train,y_train)\n",
        "# 打印在训练数据上的准确率\n",
        "print (\"训练数据上的准确率为：\" + str(svc.score(X_train, y_train)))\n",
        "\n",
        "# 打印在测试数据上的准确率\n",
        "print (\"测试数据上的准确率为: \" + str(svc.score(X_test, y_test)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "训练数据上的准确率为：0.9682579045257285\n",
            "测试数据上的准确率为: 0.7308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORNu6x0Me-bO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0f1ceafc-24e1-4df1-aa6d-6bf394d15383"
      },
      "source": [
        "# 利用线性支持向量机(LinearSVM)来做情感分析的预测\n",
        "from sklearn.svm import LinearSVC\n",
        "# 具体的使用方式请见： http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
        "\n",
        "# TODO: 初始化LinearSVC模型，并利用模型的fit函数来做训练并打印在训练和测试数据上的准确率，使用模型的默认参数。\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train,y_train)\n",
        "# 打印在训练数据上的准确率\n",
        "print (\"训练数据上的准确率为：\" + str(clf.score(X_train, y_train)))\n",
        "\n",
        "# 打印在测试数据上的准确率\n",
        "print (\"测试数据上的准确率为: \" + str(clf.score(X_test, y_test)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "训练数据上的准确率为：0.9675139491630502\n",
            "测试数据上的准确率为: 0.7296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ftXxs8kfBii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b1fc38b0-b0b9-4cb8-e25c-9aca00973812"
      },
      "source": [
        "# 利用神经网络模型来做情感分析的预测\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "# 具体使用方法请见：http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "\n",
        "# TODO: 初始化MLPClassifier模型，并利用模型的fit函数来做训练并打印在训练和测试数据上的准确率，设置为hidden_layer_sizes为100，\n",
        "# 并使用\"lbfgs\" solver\n",
        "mlp = MLPClassifier(solver='lbfgs',hidden_layer_sizes=100)\n",
        "mlp.fit(X_train,y_train)\n",
        "# 打印在训练数据上的准确率\n",
        "print (\"训练数据上的准确率为：\" + str(mlp.score(X_train, y_train)))\n",
        "\n",
        "# 打印在测试数据上的准确率\n",
        "print (\"测试数据上的准确率为: \" + str(mlp.score(X_test, y_test)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "训练数据上的准确率为：0.6199628022318661\n",
            "测试数据上的准确率为: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2DmuJLuYoXc",
        "colab_type": "text"
      },
      "source": [
        "对于超参数的调整，我们经常使用gridsearch，这也是工业界最常用的方法，但它的缺点是需要大量的计算，所以近年来这方面的研究也成为了重点。 其中一个比较经典的成果为Bayesian Optimization（利用贝叶斯的思路去寻找最好的超参数）。Ryan P. Adams主导的Bayesian Optimization利用高斯过程作为后验概率（posteior distribution）来寻找最优解。 https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf 在下面的练习中，我们尝试使用Bayesian Optimization工具来去寻找最优的超参数。参考工具：https://github.com/fmfn/BayesianOptimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSVRJgF5fKfR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aaafe8ba-d7e4-4f54-eb76-8bae1c700d9b"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold\n",
        "params_c = np.logspace(-3,3,7)   # 对于参数 “C”，尝试几个不同的值\n",
        "best_c = params_c[0]  # 存储最好的C值\n",
        "best_acc = 0\n",
        "kf = KFold(n_splits=5,shuffle=False)\n",
        "for c in params_c:\n",
        "    # TODO： 编写交叉验证的过程，对于每一个c值，计算出在验证集中的平均准确率。 在这里，我们做5-fold交叉验证。也就是，每一次把20%\n",
        "    #   的数据作为验证集来对待，然后准确率为五次的平均值。我们把这个准确率命名为 acc_avg\n",
        "    avg = 0\n",
        "    for train_index, test_index in kf.split(X_train):\n",
        "        lr = LogisticRegression(C=c).fit(X_train[train_index],y_train[train_index])\n",
        "        avg += lr.score(X_train[test_index],y_train[test_index])\n",
        "    acc_avg = avg/5\n",
        "    if acc_avg > best_acc:\n",
        "        best_acc = acc_avg\n",
        "        best_c = c\n",
        "\n",
        "print (\"最好的参数C值为： %f\" % (best_c))\n",
        "# TODO 我们需要在整个训练数据上重新训练模型，但这次利用最好的参数best_c值\n",
        "#     提示： model = LogisticRegression(C=best_c).fit(X_train, y_train)\n",
        "lr = LogisticRegression(C=best_c).fit(X_train, y_train)\n",
        "\n",
        "# 打印在训练数据上的准确率\n",
        "print (\"训练数据上的准确率为：\" + str(lr.score(X_train, y_train)))\n",
        "\n",
        "# 打印在测试数据上的准确率\n",
        "print (\"测试数据上的准确率为: \" + str(lr.score(X_test, y_test)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "最好的参数C值为： 10.000000\n",
            "训练数据上的准确率为：0.9650340979541228\n",
            "测试数据上的准确率为: 0.734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si2vW_y_YoXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: 仍然使用SVM模型，但在这里使用Bayesian Optimization来寻找最好的超参数。 \n",
        "#       1. 评估方式： F1-score\n",
        "#       2. 超参数（hyperparater）的选择利用Bayesian Optimization https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
        "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
        "#       SVM的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "#       对于SVM模型，经常调整的超参数为：C, gamma, kernel\n",
        "#       参考Bayesian Optimization开源工具： https://github.com/fmfn/BayesianOptimization\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGhNv7gUfQl4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "839ebd32-68ca-49f4-c883-93ce85c3417c"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "\n",
        "param_hidden_layer_sizes = np.linspace(10, 200, 20)  # 针对参数 “hidden_layer_sizes”, 尝试几个不同的值\n",
        "param_alphas = np.logspace(-4,1,6)  # 对于参数 \"alpha\", 尝试几个不同的值\n",
        "\n",
        "best_hidden_layer_size = param_hidden_layer_sizes[0]\n",
        "best_alpha = param_alphas[0]\n",
        "\n",
        "for size in param_hidden_layer_sizes:\n",
        "    for val in param_alphas:\n",
        "        # TODO 编写交叉验证的过程，需要做5-fold交叉验证。\n",
        "        avg = 0\n",
        "        for train_index, test_index in kf.split(X_train, y_train):\n",
        "            mlp = MLPClassifier(alpha=int(val),hidden_layer_sizes=int(size))\n",
        "            mlp.fit(X_train[train_index],y_train[train_index])\n",
        "            avg += mlp.score(X_train[test_index],y_train[test_index])\n",
        "        acc_avg = avg/5\n",
        "        if acc_avg > best_acc:\n",
        "            best_acc = acc_avg\n",
        "            best_hidden_layer_size = size\n",
        "            best_alpha = val\n",
        "\n",
        "print (\"最好的参数hidden_layer_size值为： %f\" % (best_hidden_layer_size))\n",
        "print (\"最好的参数alpha值为： %f\" % (best_alpha))\n",
        "\n",
        "# TODO 我们需要在整个训练数据上重新训练模型，但这次使用最好的参数hidden_layer_size和best_alpha\n",
        "mlp = MLPClassifier(alpha=best_alpha,hidden_layer_sizes=best_hidden_layer_size).fit(X_train,y_train)\n",
        "# 打印在训练数据上的准确率\n",
        "print (\"训练数据上的准确率为：\" + str(mlp.score(X_train, y_train)))\n",
        "\n",
        "# 打印在测试数据上的准确率\n",
        "print (\"测试数据上的准确率为: \" + str(mlp.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "TGpz7hJUYoXh",
        "colab_type": "text"
      },
      "source": [
        "### 特征: 添加n-gram特征 (10分)\n",
        "在原有tf-idf特征的基础上，添加n-gram特征（在这里我们使用bi-gram特征）。添加完之后效果是否有提升？ 为什么？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5bxqy8UYoXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train =  # 添加完bigram之后的特征\n",
        "y_train =  # \n",
        "X_test =   # 添加完bigram之后的特征\n",
        "y_test =   # \n",
        "\n",
        "print (np.shape(X_train), np.shape(X_test), np.shape(y_train), np.shape(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81qpXXIWYoXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TODO 模型的训练，如上"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "starter_code(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RdpGcO25O3G",
        "colab_type": "text"
      },
      "source": [
        "## 注意(attention)！开始做前必读项！\n",
        "\n",
        "- 所有的代码一定要在这个文件里面编写，不要自己创建一个新的文件\n",
        "- 对于提供的数据集，不要改存储地方，也不要修改文件名和内容\n",
        "- 确保到时候git pull之后我们可以直接运行这个 starter_code文件\n",
        "- 不要重新定义函数（如果我们已经定义好的），按照里面的思路来编写。当然，除了我们定义的部分，如有需要可以自行定义函数或者模块\n",
        "- 写完之后，重新看一下哪一部分比较慢，然后试图去优化。一个好的习惯是每写一部分就思考这部分代码的时间复杂度和空间复杂度，AI工程是的日常习惯！\n",
        "- 第一次作业很重要，一定要完成！ 相信会有很多的收获！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "130sQ0sE5O3H",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: 搭建一个分词工具\n",
        "\n",
        "### Part 1.1  基于枚举方法来搭建中文分词工具\n",
        "\n",
        "此项目需要的数据：\n",
        "1. 综合类中文词库.xlsx： 包含了中文词，当做词典来用\n",
        "2. 以变量的方式提供了部分unigram概率 word_prob\n",
        "\n",
        "\n",
        "举个例子： 给定词典=[我们 学习 人工 智能 人工智能 未来 是]， 另外我们给定unigram概率：p(我们)=0.25, p(学习)=0.15, p(人工)=0.05, p(智能)=0.1, p(人工智能)=0.2, p(未来)=0.1, p(是)=0.15\n",
        "\n",
        "#### Step 1: 对于给定字符串：”我们学习人工智能，人工智能是未来“, 找出所有可能的分割方式\n",
        "- [我们，学习，人工智能，人工智能，是，未来]\n",
        "- [我们，学习，人工，智能，人工智能，是，未来]\n",
        "- [我们，学习，人工，智能，人工，智能，是，未来]\n",
        "- [我们，学习，人工智能，人工，智能，是，未来]\n",
        ".......\n",
        "\n",
        "\n",
        "#### Step 2: 我们也可以计算出每一个切分之后句子的概率\n",
        "- p(我们，学习，人工智能，人工智能，是，未来)= -log p(我们)-log p(学习)-log p(人工智能)-log p(人工智能)-log p(是)-log p(未来)\n",
        "- p(我们，学习，人工，智能，人工智能，是，未来)=-log p(我们)-log p(学习)-log p(人工)-log p(智能)-log p(人工智能)-log p(是)-log p(未来)\n",
        "- p(我们，学习，人工，智能，人工，智能，是，未来)=-log p(我们)-log p(学习)-log p(人工)-log p(智能)-log p(人工)-log p(智能)-log p(是)-log p(未来)\n",
        "- p(我们，学习，人工智能，人工，智能，是，未来)=-log p(我们)-log p(学习)-log p(人工智能)-log p(人工)-log p(智能)-log(是)-log p(未来)\n",
        ".....\n",
        "\n",
        "#### Step 3: 返回第二步中概率最大的结果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWvD_XuBoGow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmLrYgUb-K-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        " \n",
        "import xlrd\n",
        "import os\n",
        " \n",
        "o_path = os.getcwd().encode('utf-8')\n",
        " \n",
        "workbook = xlrd.open_workbook('综合类中文词库.xlsx')\n",
        "table = workbook.sheet_by_index(0)\n",
        "nrows = table.nrows\n",
        "ncols = table.ncols\n",
        "\n",
        "excel_list=[]\n",
        "for row in range(nrows):\n",
        "  cell_value = table.cell(row, 0).value\n",
        "  # excel_rows = cell_value.encode('UTF-8')\n",
        "  excel_list.append(cell_value)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XohDl3RFh_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "782267b5-1446-4a4a-d300-75949d2ab749"
      },
      "source": [
        "table.cell(0, 0).value"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'酢'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEiNFH_55O3I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7a68edf-1437-4be0-bfc8-42477cba6287"
      },
      "source": [
        "# TODO: 第一步： 从dic.txt中读取所有中文词。\n",
        "#  hint: 思考一下用什么数据结构来存储这个词典会比较好？ 要考虑我们每次查询一个单词的效率。 \n",
        "dic_words = excel_list   # 保存词典库中读取的单词\n",
        "\n",
        "# 以下是每一个单词出现的概率。为了问题的简化，我们只列出了一小部分单词的概率。 在这里没有出现的的单词但是出现在词典里的，统一把概率设置成为0.00001\n",
        "# 比如 p(\"学院\")=p(\"概率\")=...0.00001\n",
        "\n",
        "word_prob = {\"北京\":0.03,\"的\":0.08,\"天\":0.005,\"气\":0.005,\"天气\":0.06,\"真\":0.04,\"好\":0.05,\"真好\":0.04,\"啊\":0.01,\"真好啊\":0.02, \n",
        "             \"今\":0.01,\"今天\":0.07,\"课程\":0.06,\"内容\":0.06,\"有\":0.05,\"很\":0.03,\"很有\":0.04,\"意思\":0.06,\"有意思\":0.005,\"课\":0.01,\n",
        "             \"程\":0.005,\"经常\":0.08,\"意见\":0.08,\"意\":0.01,\"见\":0.005,\"有意见\":0.02,\"分歧\":0.04,\"分\":0.02, \"歧\":0.005}\n",
        "\n",
        "print (sum(word_prob.values()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ayQKK1kT0oF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_break(s, dic):\n",
        "    def sentences(cur):\n",
        "        result=[]\n",
        "        if cur <len(s):\n",
        "            for next in range(cur+1, len(s)+1):\n",
        "                if s[cur:next] in dic:\n",
        "                    result = result+[s[cur:next]+(tail and ','+tail) for tail in sentences(next)]\n",
        "        else:\n",
        "            return ['']\n",
        "        return result\n",
        "\n",
        "    list_new = []\n",
        "    for line in sentences(0):\n",
        "        line = line.split(\",\")\n",
        "        list_new.append(line)\n",
        "    return list_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA-SuHzW5O3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  分数（10）\n",
        "## TODO 请编写word_segment_naive函数来实现对输入字符串的分词\n",
        "def word_segment_naive(input_str):\n",
        "    \"\"\"\n",
        "    1. 对于输入字符串做分词，并返回所有可行的分词之后的结果。\n",
        "    2. 针对于每一个返回结果，计算句子的概率\n",
        "    3. 返回概率最高的最作为最后结果\n",
        "    \n",
        "    input_str: 输入字符串   输入格式：“今天天气好”\n",
        "    best_segment: 最好的分词结果  输出格式：[\"今天\"，\"天气\"，\"好\"]\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO： 第一步： 计算所有可能的分词结果，要保证每个分完的词存在于词典里，这个结果有可能会非常多。\n",
        "    segments = word_break(input_str, dic_words)\n",
        "\n",
        "      # 存储所有分词的结果。如果次字符串不可能被完全切分，则返回空列表(list)\n",
        "      # 格式为：segments = [[\"今天\"，“天气”，“好”],[\"今天\"，“天“，”气”，“好”],[\"今“，”天\"，“天气”，“好”],...]\n",
        "    \n",
        "    # TODO: 第二步：循环所有的分词结果，并计算出概率最高的分词结果，并返回\n",
        "    scores = []\n",
        "    for segment in segments:\n",
        "      score = 0\n",
        "      for word_seg in segment:\n",
        "        if word_seg in [prob for prob in word_prob.keys()]: \n",
        "          p = word_prob[word_seg]\n",
        "        else: \n",
        "          p = 0.00001\n",
        "        score = score - np.log(p)\n",
        "      scores.append(score)\n",
        "    \n",
        "    index = scores.index(min(scores))\n",
        "    best_segment = segments[index]\n",
        "    best_score = scores[index]\n",
        "    \n",
        "    return best_segment      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m77LMmnD5O3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c4ee984c-e7d8-46c3-8f04-58225f7f3de4"
      },
      "source": [
        "# 测试\n",
        "print (word_segment_naive(\"北京的天气真好啊\"))\n",
        "print (word_segment_naive(\"今天的课程内容很有意思\"))\n",
        "print (word_segment_naive(\"经常有意见分歧\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['北京', '的', '天气', '真好', '啊']\n",
            "['今天', '的', '课程', '内容', '很', '有意思']\n",
            "['经常', '有', '意见', '分歧']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyqscjvC5O3T",
        "colab_type": "text"
      },
      "source": [
        "### Part 1.2  基于维特比算法来优化上述流程\n",
        "\n",
        "此项目需要的数据：\n",
        "1. 综合类中文词库.xlsx： 包含了中文词，当做词典来用\n",
        "2. 以变量的方式提供了部分unigram概率word_prob\n",
        "\n",
        "\n",
        "举个例子： 给定词典=[我们 学习 人工 智能 人工智能 未来 是]， 另外我们给定unigram概率：p(我们)=0.25, p(学习)=0.15, p(人工)=0.05, p(智能)=0.1, p(人工智能)=0.2, p(未来)=0.1, p(是)=0.15\n",
        "\n",
        "#### Step 1: 根据词典，输入的句子和 word_prob来创建带权重的有向图（Directed Graph） 参考：课程内容\n",
        "有向图的每一条边是一个单词的概率（只要存在于词典里的都可以作为一个合法的单词），这些概率已经给出（存放在word_prob）。\n",
        "注意：思考用什么方式来存储这种有向图比较合适？ 不一定只有一种方式来存储这种结构。 \n",
        "\n",
        "#### Step 2: 编写维特比算法（viterebi）算法来找出其中最好的PATH， 也就是最好的句子切分\n",
        "具体算法参考课程中讲过的内容\n",
        "\n",
        "#### Step 3: 返回结果\n",
        "跟PART 1.1的要求一致"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYbYzRTs5O3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分数（10）\n",
        "\n",
        "## TODO 请编写word_segment_viterbi函数来实现对输入字符串的分词\n",
        "def word_segment_viterbi(input_str):\n",
        "    \"\"\"\n",
        "    1. 基于输入字符串，词典，以及给定的unigram概率来创建DAG(有向图）。\n",
        "    2. 编写维特比算法来寻找最优的PATH\n",
        "    3. 返回分词结果\n",
        "    \n",
        "    input_str: 输入字符串   输入格式：“今天天气好”\n",
        "    best_segment: 最好的分词结果  输出格式：[\"今天\"，\"天气\"，\"好\"]\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO: 第一步：根据词典，输入的句子，以及给定的unigram概率来创建带权重的有向图（Directed Graph） 参考：课程内容\n",
        "    #      有向图的每一条边是一个单词的概率（只要存在于词典里的都可以作为一个合法的单词），这些概率在 word_prob，如果不在word_prob里的单词但在\n",
        "    #      词典里存在的，统一用概率值0.00001。\n",
        "    #      注意：思考用什么方式来存储这种有向图比较合适？ 不一定有只有一种方式来存储这种结构。 \n",
        "    graph = {}\n",
        "    N = len(input_str)\n",
        "    for i in range(N,0,-1):\n",
        "        k=i-1\n",
        "        in_list=[]\n",
        "        flag = input_str[k:i]\n",
        "        while k>=0 and flag in dic_words:\n",
        "            in_list.append(k)\n",
        "            k -= 1\n",
        "            flag = input_str[k:i]\n",
        "        graph[i] = in_list\n",
        "\n",
        "    \n",
        "    # TODO： 第二步： 利用维特比算法来找出最好的PATH， 这个PATH是P(sentence)最大或者 -log P(sentence)最小的PATH。\n",
        "    #              hint: 思考为什么不用相乘: p(w1)p(w2)...而是使用negative log sum:  -log(w1)-log(w2)-...\n",
        "    mem=[0]* (N+1)\n",
        "    last_index=[0]*(N+1)\n",
        "    for i in range(1,N+1):\n",
        "        min_dis=math.inf\n",
        "        for j in graph[i]:\n",
        "            if input_str[j:i] in word_prob.keys():\n",
        "                #      有向图的每一条边是一个单词的概率（只要存在于词典里的都可以作为一个合法的单词），这些概率在 word_prob，如果不在word_prob里的单词但在\n",
        "                #      词典里存在的，统一用概率值0.00001。\n",
        "                if min_dis > mem[j]+round(-math.log(word_prob[input_str[j:i]]),1):\n",
        "                    min_dis=mem[j]+round(-math.log(word_prob[input_str[j:i]]),1)\n",
        "                    last_index[i]=j\n",
        "            else:\n",
        "                if min_dis > mem[j]+round(-math.log(0.00001),1):\n",
        "                    min_dis=mem[j]+round(-math.log(0.00001),1)\n",
        "                    last_index[i]=j\n",
        "        mem[i]=min_dis\n",
        "\n",
        "    \n",
        "    # TODO: 第三步： 根据最好的PATH, 返回最好的切分\n",
        "    best_segment=[]\n",
        "    j=N\n",
        "    while True:\n",
        "        best_segment.append(input_str[last_index[j]:j])\n",
        "        j=last_index[j]\n",
        "        if j==0 and last_index[j]==0:\n",
        "            break\n",
        "    best_segment.reverse()\n",
        "\n",
        "    return best_segment      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Q_gx1i5O3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8f762d4e-f7b4-4013-832a-0839d5988426"
      },
      "source": [
        "# 测试\n",
        "print (word_segment_naive(\"北京的天气真好啊\"))\n",
        "print (word_segment_naive(\"今天的课程内容很有意思\"))\n",
        "print (word_segment_naive(\"经常有意见分歧\"))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['北京', '的', '天气', '真好', '啊']\n",
            "['今天', '的', '课程', '内容', '很', '有意思']\n",
            "['经常', '有', '意见', '分歧']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnNPkrDC5O3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分数（3）\n",
        "# TODO: 第一种方法和第二种方法的时间复杂度和空间复杂度分别是多少？\n",
        "第一个方法： \n",
        "时间复杂度= , 空间复杂度=\n",
        "\n",
        "第二个方法：\n",
        "时间复杂度= , 空间复杂度="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTXNmYsx5O3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分数（2）\n",
        "# TODO：如果把上述的分词工具持续优化，有哪些可以考虑的方法？ （至少列出3点）\n",
        "- 0. （例）， 目前的概率是不完整的，可以考虑大量的语料库，然后从中计算出每一个词出现的概率，这样更加真实\n",
        "- 1.\n",
        "- 2.\n",
        "- 3. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "I81twd3c5O3g",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "52ECPOHD5O3h",
        "colab_type": "text"
      },
      "source": [
        "## Part 2:  搭建一个简单的问答系统"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgNDnrSy5O3i",
        "colab_type": "text"
      },
      "source": [
        "本次项目的目标是搭建一个基于检索式的简单的问答系统。至于什么是检索式的问答系统请参考课程直播内容/PPT介绍。 \n",
        "\n",
        "通过此项目，你将会有机会掌握以下几个知识点：\n",
        "1. 字符串操作   2. 文本预处理技术（词过滤，标准化）   3. 文本的表示（tf-idf, word2vec)  4. 文本相似度计算  5. 文本高效检索\n",
        "\n",
        "此项目需要的数据：\n",
        "1. dev-v2.0.json: 这个数据包含了问题和答案的pair， 但是以JSON格式存在，需要编写parser来提取出里面的问题和答案。 \n",
        "2. glove.6B: 这个文件需要从网上下载，下载地址为：https://nlp.stanford.edu/projects/glove/， 请使用d=100的词向量\n",
        "\n",
        "##### 检索式的问答系统\n",
        "问答系统所需要的数据已经提供，对于每一个问题都可以找得到相应的答案，所以可以理解为每一个样本数据是 <问题、答案>。 那系统的核心是当用户输入一个问题的时候，首先要找到跟这个问题最相近的已经存储在库里的问题，然后直接返回相应的答案即可。 举一个简单的例子：\n",
        "\n",
        "假设我们的库里面已有存在以下几个<问题,答案>：\n",
        "<\"贪心学院主要做什么方面的业务？”， “他们主要做人工智能方面的教育”>\n",
        "<“国内有哪些做人工智能教育的公司？”， “贪心学院”>\n",
        "<\"人工智能和机器学习的关系什么？\", \"其实机器学习是人工智能的一个范畴，很多人工智能的应用要基于机器学习的技术\">\n",
        "<\"人工智能最核心的语言是什么？\"， ”Python“>\n",
        ".....\n",
        "\n",
        "假设一个用户往系统中输入了问题 “贪心学院是做什么的？”， 那这时候系统先去匹配最相近的“已经存在库里的”问题。 那在这里很显然是 “贪心学院是做什么的”和“贪心学院主要做什么方面的业务？”是最相近的。 所以当我们定位到这个问题之后，直接返回它的答案 “他们主要做人工智能方面的教育”就可以了。 所以这里的核心问题可以归结为计算两个问句（query）之间的相似度。\n",
        "\n",
        "在本次项目中，你会频繁地使用到sklearn这个机器学习库。具体安装请见：http://scikit-learn.org/stable/install.html  sklearn包含了各类机器学习算法和数据处理工具，包括本项目需要使用的词袋模型，均可以在sklearn工具包中找得到。 另外，本项目还需要用到分词工具jieba, 具体使用方法请见 https://github.com/fxsjy/jieba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVKPEWdd5O3i",
        "colab_type": "text"
      },
      "source": [
        "### Part 2.1  第一部分： 读取文件，并把内容分别写到两个list里（一个list对应问题集，另一个list对应答案集）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgfQ_RtU5O3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "# 分数（5）\n",
        "def read_corpus():\n",
        "    \"\"\"\n",
        "    读取给定的语料库，并把问题列表和答案列表分别写入到 qlist, alist 里面。 在此过程中，不用对字符换做任何的处理（这部分需要在 Part 2.3里处理）\n",
        "    qlist = [\"问题1\"， “问题2”， “问题3” ....]\n",
        "    alist = [\"答案1\", \"答案2\", \"答案3\" ....]\n",
        "    务必要让每一个问题和答案对应起来（下标位置一致）\n",
        "    \"\"\"\n",
        "    qlist = []\n",
        "    alist = []\n",
        "    with open('train-v2.0.json') as f:\n",
        "        data_array = json.load(f)['data']\n",
        "        for data in data_array:\n",
        "            paragraphs = data['paragraphs']\n",
        "            for paragraph in paragraphs:\n",
        "                qas = paragraph['qas']\n",
        "                for qa in qas:\n",
        "                    if 'plausible_answers' in qa:\n",
        "                        qlist.append(qa['question'])\n",
        "                        alist.append(qa['plausible_answers'][0]['text'])\n",
        "                    else:\n",
        "                        qlist.append(qa['question'])\n",
        "                        alist.append(qa['answers'][0]['text'])\n",
        "\n",
        "\n",
        "    assert len(qlist) == len(alist)  # 确保长度一样\n",
        "    return qlist, alist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOrXKe365O3m",
        "colab_type": "text"
      },
      "source": [
        "### Part 2.2 理解数据（可视化分析/统计信息）\n",
        "对数据的理解是任何AI工作的第一步，需要充分对手上的数据有个更直观的理解。 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ery_YqpT5O3n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a2e0ab12-10ac-44b7-befe-5862cbde25cb"
      },
      "source": [
        "# 分数（10）\n",
        "# TODO: 统计一下在qlist 总共出现了多少个单词？ 总共出现了多少个不同的单词？\n",
        "#       这里需要做简单的分词，对于英文我们根据空格来分词即可，其他过滤暂不考虑（只需分词）\n",
        "# TODO: 统计一下在qlist 总共出现了多少个单词？ 总共出现了多少个不同的单词？\n",
        "#       这里需要做简单的分词，对于英文我们根据空格来分词即可，其他过滤暂不考虑（只需分词）\n",
        "from collections import Counter\n",
        "qlist , alist = read_corpus()\n",
        "# 分词\n",
        "def cut(input_list):\n",
        "    list_new = []\n",
        "    for q in input_list:\n",
        "        list_new.append(q.replace('?','').split(' '))\n",
        "    return list_new\n",
        "qlist_new = [q for l in cut(qlist) for q in l]\n",
        "dif_word_total = len(qlist_new)\n",
        "word_dict = Counter(qlist_new)\n",
        "word_total = len(dict(word_dict))\n",
        "print (\"一共出现了 %d 个单词\"%dif_word_total)\n",
        "print (\"共有 %d 个不同的单词\"%word_total)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "一共出现了 1321965 个单词\n",
            "共有 61885 个不同的单词\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0iyHvVD5O3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "913eb52e-8c00-49d0-d193-2e0695406f3b"
      },
      "source": [
        "# TODO: 统计一下qlist中每个单词出现的频率，并把这些频率排一下序，然后画成plot. 比如总共出现了总共7个不同单词，而且每个单词出现的频率为 4, 5,10,2, 1, 1,1\n",
        "#       把频率排序之后就可以得到(从大到小) 10, 5, 4, 2, 1, 1, 1. 然后把这7个数plot即可（从大到小）\n",
        "#       需要使用matplotlib里的plot函数。y轴是词频\n",
        "import matplotlib.pyplot as plt\n",
        "y = []\n",
        "for i in word_dict:\n",
        "    y.append(word_dict[i])\n",
        "\n",
        "plt.plot(sorted(y,reverse=True)[50:])\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBMNXcU65O3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO： 从上面的图中能观察到什么样的现象？ 这样的一个图的形状跟一个非常著名的函数形状很类似，能所出此定理吗？ \n",
        "#       hint: [XXX]'s law\n",
        "# \n",
        "# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FydgHT5T5O3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: 在qlist和alist里出现次数最多的TOP 10单词分别是什么？ \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIoMqYD55O3y",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 文本预处理\n",
        "次部分需要尝试做文本的处理。在这里我们面对的是英文文本，所以任何对英文适合的技术都可以考虑进来。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms3h6mFZ5O3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分数（10）\n",
        "\n",
        "# TODO: 对于qlist, alist做文本预处理操作。 可以考虑以下几种操作：\n",
        "#       1. 停用词过滤 （去网上搜一下 \"english stop words list\"，会出现很多包含停用词库的网页，或者直接使用NLTK自带的）   \n",
        "#       2. 转换成lower_case： 这是一个基本的操作   \n",
        "#       3. 去掉一些无用的符号： 比如连续的感叹号！！！， 或者一些奇怪的单词。\n",
        "#       4. 去掉出现频率很低的词：比如出现次数少于10,20....\n",
        "#       5. 对于数字的处理： 分词完只有有些单词可能就是数字比如44，415，把所有这些数字都看成是一个单词，这个新的单词我们可以定义为 \"#number\"\n",
        "#       6. stemming（利用porter stemming): 因为是英文，所以stemming也是可以做的工作\n",
        "#       7. 其他（如果有的话）\n",
        "#       请注意，不一定要按照上面的顺序来处理，具体处理的顺序思考一下，然后选择一个合理的顺序\n",
        "#  hint: 停用词用什么数据结构来存储？不一样的数据结构会带来完全不一样的效率！  \n",
        "\n",
        "import string\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# 低频词库\n",
        "low_frequency_words = []\n",
        "for (k,v) in  word_dict.items():\n",
        "    if v < 2:\n",
        "        low_frequency_words.append(k)\n",
        "p = PorterStemmer()\n",
        "\n",
        "def text_preprocessing(input_list):\n",
        "    stop_words = [\"a\",\"able\",\"about\",\"across\",\"after\",\"all\",\"almost\",\"also\",\"am\",\"among\",\"an\",\"and\",\"any\",\"are\",\"as\",\"at\",\"be\",\"because\",\"been\",\"but\",\"by\",\"can\",\"cannot\",\"could\",\"dear\",\"did\",\"do\",\"does\",\"either\",\"else\",\"ever\",\"every\",\"for\",\"from\",\"get\",\"got\",\"had\",\"has\",\"have\",\"he\",\"her\",\"hers\",\"him\",\"his\",\"how\",\"however\",\"i\",\"if\",\"in\",\"into\",\"is\",\"it\",\"its\",\"just\",\"least\",\"let\",\"like\",\"likely\",\"may\",\"me\",\"might\",\"most\",\"must\",\"my\",\"neither\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"often\",\"on\",\"only\",\"or\",\"other\",\"our\",\"own\",\"rather\",\"said\",\"say\",\"says\",\"she\",\"should\",\"since\",\"so\",\"some\",\"than\",\"that\",\"the\",\"their\",\"them\",\"then\",\"there\",\"these\",\"they\",\"this\",\"tis\",\"to\",\"too\",\"twas\",\"us\",\"wants\",\"was\",\"we\",\"were\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"would\",\"yet\",\"you\",\"your\",\"ain't\",\"aren't\",\"can't\",\"could've\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\"hasn't\",\"he'd\",\"he'll\",\"he's\",\"how'd\",\"how'll\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"isn't\",\"it's\",\"might've\",\"mightn't\",\"must've\",\"mustn't\",\"shan't\",\"she'd\",\"she'll\",\"she's\",\"should've\",\"shouldn't\",\"that'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"wasn't\",\"we'd\",\"we'll\",\"we're\",\"weren't\",\"what'd\",\"what's\",\"when'd\",\"when'll\",\"when's\",\"where'd\",\"where'll\",\"where's\",\"who'd\",\"who'll\",\"who's\",\"why'd\",\"why'll\",\"why's\",\"won't\",\"would've\",\"wouldn't\",\"you'd\",\"you'll\",\"you're\",\"you've\"]\n",
        "    # 分词\n",
        "    input_list = cut(input_list)    \n",
        "    new_list = [] #保存处理完的qlist\\alist\n",
        "    for l in input_list:\n",
        "        l_list = '' # 保存句子\n",
        "        for word in l:\n",
        "            # 转换小写/stemming\n",
        "            word = p.stem(word)\n",
        "            \n",
        "            # 去除所有标点符号\n",
        "            word = ''.join(c for c in word if c not in string.punctuation)\n",
        "            \n",
        "            # 处理数字\n",
        "            if word.isdigit():\n",
        "                word = word.replace(word,'#number')\n",
        "            \n",
        "            if word not in stop_words and word not in low_frequency_words:\n",
        "                l_list += word + ' '\n",
        "        new_list.append(l_list)\n",
        "    return new_list\n",
        "       \n",
        "qlist = text_preprocessing(qlist)   # 更新后的"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2RaM5R45O32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: 在前面步骤里，我们删除了出现次数比较少的单词，那你选择的阈值是多少（小于多少的去掉？）， 这个阈值是根据什么来选择的？ \n",
        "# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoZ90SDd5O37",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 文本表示\n",
        "当我们做完关键的预处理过程之后，就需要把每一个文本转换成向量。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdOeDQmR5O38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分数（10）\n",
        "\n",
        "# TODO: 把qlist中的每一个问题字符串转换成tf-idf向量, 转换之后的结果存储在X矩阵里。 X的大小是： N* D的矩阵。 这里N是问题的个数（样本个数），\n",
        "#       D是字典库的大小。 \n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer() # 定一个tf-idf的vectorizer\n",
        "X = vectorizer.fit_transform(qlist)   # 结果存放在X矩阵\n",
        "\n",
        "X_list = [i for k in X.toarray() for i in k]\n",
        "count_total = len(X_list)\n",
        "count_zero = X_list.count(0)\n",
        "sparsity = (count_total-count_zero)/count_total\n",
        "print (sparsity)  # 打印出稀疏度"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP1cxD_85O3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: 矩阵X有什么特点？ 计算一下它的稀疏度\n",
        "\n",
        "print (sparsity)  # 打印出稀疏度(sparsity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmm-a9M75O4C",
        "colab_type": "text"
      },
      "source": [
        "### 2.5 对于用户的输入问题，找到相似度最高的TOP5问题，并把5个潜在的答案做返回"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vldgbngE5O4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分数（10）\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "alist = np.array(alist)\n",
        "def top5results(input_q):\n",
        "    \"\"\"\n",
        "    给定用户输入的问题 input_q, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
        "    1. 对于用户的输入 input_q 首先做一系列的预处理，然后再转换成tf-idf向量（利用上面的vectorizer)\n",
        "    2. 计算跟每个库里的问题之间的相似度\n",
        "    3. 找出相似度最高的top5问题的答案\n",
        "    \"\"\"\n",
        "    input_q = text_preprocessing([input_q])\n",
        "    input_vec = vectorizer.transform(input_q)\n",
        "    res = cosine_similarity(input_vec,X)[0]\n",
        "    top_idxs = np.argsort(res)[-5:].tolist()  # top_idxs存放相似度最高的（存在qlist里的）问题的下标\n",
        "    return alist[top_idxs]  # 返回相似度最高的问题对应的答案，作为TOP5答案 \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0JeZWey5O4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: 编写几个测试用例，并输出结果\n",
        "print (top5results(\"\"))\n",
        "print (top5results(\"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahCOLEsp5O4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分数（5）\n",
        "\n",
        "# TODO: 上面的top5results算法的时间复杂度和空间复杂度分别是多少？\n",
        "\n",
        "时间复杂度 = O()， 空间复杂度 = O()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4pQ8TZO5O4J",
        "colab_type": "text"
      },
      "source": [
        "### 2.6 利用倒排表的优化。 \n",
        "上面的算法，一个最大的缺点是每一个用户问题都需要跟库里的所有的问题都计算相似度。假设我们库里的问题非常多，这将是效率非常低的方法。 这里面一个方案是通过倒排表的方式，先从库里面找到跟当前的输入类似的问题描述。然后针对于这些candidates问题再做余弦相似度的计算。这样会节省大量的时间。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmkOLjQY5O4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分数（10）\n",
        "\n",
        "# TODO: 基于倒排表的优化。在这里，我们可以定义一个类似于hash_map, 比如 inverted_index = {}， 然后存放包含每一个关键词的文档出现在了什么位置，\n",
        "#       也就是，通过关键词的搜索首先来判断包含这些关键词的文档（比如出现至少一个），然后对于candidates问题做相似度比较。\n",
        "# \n",
        "    \"\"\"\n",
        "    给定用户输入的问题 input_q, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
        "    1. 利用倒排表来筛选 candidate\n",
        "    2. 对于用户的输入 input_q 首先做一系列的预处理，然后再转换成tf-idf向量（利用上面的vectorizer)\n",
        "    3. 计算跟每个库里的问题之间的相似度\n",
        "    4. 找出相似度最高的top5问题的答案\n",
        "    \"\"\"\n",
        "\n",
        "inverted_idx = {}  # 定一个一个简单的倒排表\n",
        "\n",
        "# 将关键词添加到倒排表中\n",
        "for key,value in word_dict.items():\n",
        "    if value >100 and value <1000:\n",
        "        inverted_idx[key]=[]\n",
        "        \n",
        "# 在倒排表中添加关键词存在于qlist中的索引\n",
        "keywords_index = 0\n",
        "for q in cut(qlist):\n",
        "    for word in q:\n",
        "        if word in inverted_idx.keys():\n",
        "            inverted_idx[word].append(keywords_index)\n",
        "    keywords_index += 1\n",
        "\n",
        "def top5results_invidx(input_q):\n",
        "    # 从倒排表中取出相关联的索引\n",
        "    index_list = []\n",
        "    for c in cut([input_q]):\n",
        "        for i in c:\n",
        "            if i in inverted_idx.keys():\n",
        "                index_list += inverted_idx[i]\n",
        "    index_list = list(set(index_list))\n",
        "    \n",
        "    input_q = text_preprocessing([input_q])\n",
        "    input_vec = vectorizer.transform(input_q)\n",
        "    res = cosine_similarity(input_vec,X[index_list])[0]\n",
        "    top_idxs = np.argsort(res)[-5:].tolist()  # top_idxs存放相似度最高的（存在qlist里的）问题的下标"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYDOq9ep5O4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: 编写几个测试用例，并输出结果\n",
        "print (top5results_invidx(\"\"))\n",
        "print (top5results_invidx(\"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qopsy1K5O4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分数（3）\n",
        "\n",
        "# TODO: 上面的top5results算法的时间复杂度和空间复杂度分别是多少？\n",
        "\n",
        "时间复杂度 = O()， 空间复杂度 = O()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ8pJWoM5O4R",
        "colab_type": "text"
      },
      "source": [
        "### 2.7 基于词向量的文本表示\n",
        "上面所用到的方法论是基于词袋模型（bag-of-words model）。这样的方法论有两个主要的问题：1. 无法计算词语之间的相似度  2. 稀疏度很高。 在2.7里面我们\n",
        "讲采用词向量作为文本的表示。词向量方面需要下载： https://nlp.stanford.edu/projects/glove/ （请下载glove.6B.zip），并使用d=100的词向量（100维）。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYOlOCxU5O4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分数（10）\n",
        "# 建立glove矩阵\n",
        "with open('data/glove.6B.100d.txt', 'r') as file1:\n",
        "    emb = []\n",
        "    vocab = []\n",
        "    for line in file1.readlines():\n",
        "        row = line.strip().replace(\"?\",'').split(' ')\n",
        "        vocab.append(row[0])\n",
        "        emb.append(row[1:])\n",
        "emb =np.asarray(emb) # 读取每一个单词的嵌入。这个是 D*H的矩阵，这里的D是词典库的大小， H是词向量的大小。 这里面我们给定的每个单词的词向量，那句子向量怎么表达？\n",
        "      # 其中，最简单的方式 句子向量 = 词向量的平均（出现在问句里的）， 如果给定的词没有出现在词典库里，则忽略掉这个词。\n",
        "\n",
        "# 获取句子的vec值\n",
        "def get_words_vec(words):\n",
        "    glove_index_list = []\n",
        "    # 取句子对应单词的索引\n",
        "    for word in words.split(' '):\n",
        "        if word in vocab:\n",
        "            index = vocab.index(word)\n",
        "            glove_index_list.append(index)\n",
        "    return emb[glove_index_list].astype(float).sum()/len(words.split(' '))\n",
        "\n",
        "qlist_matrix = []\n",
        "for q in qlist:\n",
        "    q_vec = get_words_vec(q)\n",
        "    qlist_matrix.append(q_vec)\n",
        "qlist_matrix = np.asarray(qlist_matrix)\n",
        "\n",
        "def top5results_emb(input_q):\n",
        "    \"\"\"\n",
        "    给定用户输入的问题 input_q, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
        "    1. 利用倒排表来筛选 candidate\n",
        "    2. 对于用户的输入 input_q，转换成句子向量\n",
        "    3. 计算跟每个库里的问题之间的相似度\n",
        "    4. 找出相似度最高的top5问题的答案\n",
        "    \"\"\"   \n",
        "    abs_v = abs(qlist_matrix[0])\n",
        "    # 存储所有返回结果的索引\n",
        "    res = []\n",
        "    # 从倒排表中取出相关联的索引\n",
        "    index_list = []\n",
        "    for c in cut([input_q]):\n",
        "        for i in c:\n",
        "            if i in inverted_idx.keys():\n",
        "                index_list += inverted_idx[i]\n",
        "    index_list = list(set(index_list))\n",
        "    # input_q的vec值\n",
        "    input_q_vec = get_words_vec(input_q)\n",
        "    # 遍历倒排表内所有值，将list中所有vec值与input_q的vec值做对比，绝对值差较小的数的索引存入res中\n",
        "    for value in qlist_matrix[index_list]:\n",
        "        if abs(input_q_vec-value) < abs_v:\n",
        "            abs_v = abs(input_q_vec-value)\n",
        "            res.append(qlist_matrix[index_list].tolist().index(value))\n",
        "    \n",
        "    top_indx = np.argsort(res)[-5:].tolist()\n",
        "    \n",
        "    return alist[top_indx]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsYrxTFG5O4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: 编写几个测试用例，并输出结果\n",
        "print (top5results_emb(\"\"))\n",
        "print (top5results_emb(\"\"))\n",
        "\n",
        "# 我们在验收作业时在后台会建立几个测试用例，来验证返回的准确性。"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2gzQgOJp5O4W",
        "colab_type": "text"
      },
      "source": [
        "### 2.8 做完本次项目做完之后有什么收获？ \n",
        "\n",
        "#分数（2）\n",
        "\n",
        "回答 = “”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKw0JV105O4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}